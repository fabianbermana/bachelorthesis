{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1uvigHjTjaF6",
   "metadata": {
    "id": "1uvigHjTjaF6"
   },
   "source": [
    "### Imports and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "YOs1wQq8jW9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOs1wQq8jW9a",
    "outputId": "6d726508-1a3c-4076-f9bd-083be3ccc61b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skorch in c:\\users\\berma\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\berma\\anaconda3\\lib\\site-packages (from skorch) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\berma\\anaconda3\\lib\\site-packages (from skorch) (1.7.3)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in c:\\users\\berma\\anaconda3\\lib\\site-packages (from skorch) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\berma\\anaconda3\\lib\\site-packages (from skorch) (1.21.5)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\berma\\anaconda3\\lib\\site-packages (from skorch) (0.8.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\berma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->skorch) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\berma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->skorch) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\berma\\anaconda3\\lib\\site-packages (from tqdm>=4.14.0->skorch) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "!pip install skorch\n",
    "from skorch import NeuralNet\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from forecast_evaluation import R2OS, CW_test, MSE, MAE, R2LOG, DM_test\n",
    "from sklearn import metrics\n",
    "\n",
    "display = ['out-of-sample R2', 'MSPE_bmk', 'losses', 'CW_test', 'elapsed time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11263f56",
   "metadata": {
    "id": "11263f56"
   },
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y, window_size):\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        x = self.tssplit(x)\n",
    "        self.x = torch.from_numpy(x).type(torch.float32)\n",
    "        self.y = torch.from_numpy(y).type(torch.float32)\n",
    "        self.length = x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def tssplit(self, data):\n",
    "        window_size = self.window_size\n",
    "        X = np.zeros((data.shape[0]-window_size, window_size, data.shape[1]))\n",
    "        for i in range(data.shape[0]-window_size):\n",
    "            X[i,:] = data[i:i+window_size,:]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce3d5da",
   "metadata": {
    "id": "6ce3d5da"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.droput = dropout\n",
    "        # input_size shows the number of features for each timestep.\n",
    "        # we don't have to explicitly say how many sequences we'll have\n",
    "        # input of RNN: #batches  x time_seq x # n_features\n",
    "        self.rnn = nn.RNN(input_size,\n",
    "                          hidden_size,\n",
    "                          num_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout)\n",
    "        # we will concatenate all sequences from each timestep and send it\n",
    "        # to the linear layer.\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # initialize hidden state\n",
    "        # x.size(0) is the batch size\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # we don't need to store hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # pass all training examples, last hidden state, all features\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.droput = dropout\n",
    "        # input_size shows the number of features for each timestep.\n",
    "        # we don't have to explicitly say how many sequences we'll have\n",
    "        # input of RNN: #batches  x time_seq x # n_features\n",
    "        self.lstm = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout)\n",
    "        # we will concatenate all sequences from each timestep and send it\n",
    "        # to the linear layer.\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # initialize hidden state\n",
    "        # x.size(0) is the batch size\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # we don't need to store hidden state\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # pass all training examples, last hidden state, all features\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.droput = dropout\n",
    "        # input_size shows the number of features for each timestep.\n",
    "        # we don't have to explicitly say how many sequences we'll have\n",
    "        # input of RNN: #batches  x time_seq x # n_features\n",
    "        self.rnn = nn.RNN(input_size,\n",
    "                          hidden_size,\n",
    "                          num_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout)\n",
    "        # we will concatenate all sequences from each timestep and send it\n",
    "        # to the linear layer.\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # initialize hidden state\n",
    "        # x.size(0) is the batch size\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # we don't need to store hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # pass all training examples, last hidden state, all features\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9629ba71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "9629ba71",
    "outputId": "14c54173-042b-4819-8d21-82cd0ce4764a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n\\nmodel_args = {\\n    'module': RNN,\\n    'module__input_size': 13,\\n    'module__hidden_size': 10,\\n    'module__num_layers': 2,\\n    'module__dropout': 0.5,\\n    'criterion': torch.nn.MSELoss,\\n    'optimizer': optim.SGD,\\n    'max_epochs': 10,\\n    'batch_size': 36,\\n    'train_split': None,\\n    'verbose': 0\\n}\\n\\ncv_args = {\\n    'module__hidden_size':[10],\\n    'module__num_layers':[2]\\n}\\n\\nresults = expanding_window_recurrent_skorch(x[0:408], \\n                                         y[0:408], \\n                                         360,\\n                                         window_size=12,\\n                                         cv=TimeSeriesSplit(2),\\n                                         model_args=model_args,\\n                                         cv_args=cv_args,\\n                                         random_seeds=None,\\n                                         verbosity=50,\\n                                         workers=-1,\\n                                         parallel=False,\\n                                         standardize=False,\\n                                         retrain_interval=12)\\n\\nfor item in display:\\n    print(f'{item}: {results[item]}')\\n    \\nresults['pred_model']\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expanding_window_recurrent_skorch(x, y, test_start, window_size, cv, model_args, cv_args, workers=-1,\n",
    "                              verbosity=10, random_seeds=None, parallel=True, standardize=True, retrain_interval=1):\n",
    "    \"\"\"\n",
    "    DESCRIPTION\n",
    "    -----------\n",
    "    Produce 1-step ahead forecasts, trained with recursive expanding windows.\n",
    "    Uses parallelization on multiple CPU cores to speed up the process.\n",
    "\n",
    "    PARAMETERS\n",
    "    -----------\n",
    "    X_train: 2-dimensional numpy array, independent variables used to train the forecast model\n",
    "\n",
    "    X_test: 2-dimensional numpy array, independent variables used for forecasting\n",
    "\n",
    "    y_train: 1-dimensional numpy array, dependent variable used to train the forecast model\n",
    "\n",
    "    y_test: 1-dimensional numpy array, dependent variable to forecast\n",
    "\n",
    "    forecast_model: python object, model used for forecasting. Make this object similar to the\n",
    "\n",
    "    sklearn models, with a fit() and predict() function\n",
    "\n",
    "    model_args: dict, arguments to pass when initializing the model\n",
    "\n",
    "    workers: int, how many processes are used. -1 denotes using all available logical processors\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # if no model arguments, parse an empty dictionary\n",
    "    if model_args is None:\n",
    "        model_args = {}\n",
    "\n",
    "    n_periods = int((x.shape[0] - test_start) / retrain_interval)\n",
    "    pred_model = np.zeros((n_periods*retrain_interval,))\n",
    "    pred_bmk = np.zeros((n_periods*retrain_interval,))\n",
    "\n",
    "    if random_seeds is not None:\n",
    "        if len(random_seeds) != n_periods:\n",
    "            raise ValueError('length of random_seeds must be same as length of X_test')\n",
    "            \n",
    "    if (x.shape[0] - test_start) % retrain_interval != 0:\n",
    "        raise ValueError('number of forecast periods is not divisible by retrain_intervals')\n",
    "\n",
    "    # function to produce recursive expanding window forecasts\n",
    "    def ExpW_forecast(x, y, i, test_start):\n",
    "\n",
    "        if random_seeds is not None:\n",
    "            torch.manual_seed(random_seeds[i])\n",
    "            random.seed(random_seeds[i])\n",
    "            np.random.seed(random_seeds[i])\n",
    "\n",
    "        # ensure safety of x and y matrices\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "        \n",
    "        # historical average forecast as the baseline\n",
    "        bmk = np.zeros((retrain_interval,1))\n",
    "        for j in range(retrain_interval):\n",
    "            bmk[j] = y[:test_start-window_size+i*retrain_interval+j].mean()\n",
    "            \n",
    "        x_std = x[0:test_start+i*retrain_interval]\n",
    "        y_std = y[0:test_start+i*retrain_interval]\n",
    "\n",
    "        if standardize:\n",
    "            # scale the data\n",
    "            x_scaler = StandardScaler().fit(x_std)\n",
    "            y_scaler = StandardScaler().fit(y_std)\n",
    "\n",
    "            x = x_scaler.transform(x)        \n",
    "            y = y_scaler.transform(y)\n",
    "        \n",
    "        dataset = StockDataset(x, y, window_size=window_size)\n",
    "        \n",
    "        # take the correct subsets of the dataset for training and testing\n",
    "        fit_subset = Subset(dataset, range(0,test_start-window_size+i*retrain_interval))\n",
    "        pred_subset = Subset(dataset, range(test_start-window_size+i*retrain_interval,test_start-window_size+(i+1)*retrain_interval))\n",
    "\n",
    "        # forecasts of model\n",
    "        forecast_model = NeuralNet(**model_args)\n",
    "        model = GridSearchCV(forecast_model, cv_args, refit=True, cv=cv, scoring='neg_mean_squared_error')\n",
    "        model = model.fit(fit_subset[:][0],fit_subset[:][1])\n",
    "        pred = model.predict(pred_subset).reshape(-1,1)\n",
    "            \n",
    "        if not parallel:\n",
    "            print(f'{i + 1}/{n_periods}', end='\\r')\n",
    "        \n",
    "        # return a tuple of the baseline and the expanding window forecasts\n",
    "        if standardize:\n",
    "            out = bmk, y_scaler.inverse_transform(pred)\n",
    "        else:\n",
    "            out = bmk, pred\n",
    "\n",
    "        return out\n",
    "\n",
    "    # run the forecast models in parallel\n",
    "    if parallel:\n",
    "        pred_pairs = Parallel(n_jobs=workers, backend='loky', verbose=verbosity) \\\n",
    "            (delayed(ExpW_forecast)(x, y, i, test_start) for i in range(n_periods))\n",
    "            \n",
    "    else:\n",
    "        pred_pairs = [ExpW_forecast(x, y, i, test_start) for i in range(n_periods)]\n",
    "\n",
    "    # split the prediction tuples\n",
    "    for i, pred_pair in enumerate(pred_pairs):\n",
    "        pred_bmk[i*retrain_interval:(i+1)*retrain_interval] = pred_pair[0].flatten()\n",
    "        pred_model[i*retrain_interval:(i+1)*retrain_interval] = pred_pair[1].flatten()\n",
    "\n",
    "    # calculate metrics\n",
    "    y_test = y[test_start:]\n",
    "    MSPE_model = metrics.mean_squared_error(y_test, pred_model)\n",
    "    MSPE_bmk = metrics.mean_squared_error(y_test, pred_bmk)\n",
    "    R2OS_val = R2OS(MSPE_model, MSPE_bmk)\n",
    "    cw_test= CW_test(y_test, pred_bmk, pred_model)\n",
    "    losses = {\n",
    "        'MSE': MSE(y_test, pred_model),\n",
    "        'MAE': MAE(y_test, pred_model),\n",
    "        'R2LOG': R2LOG(y_test, pred_model)\n",
    "    }\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    out = {'out-of-sample R2': R2OS_val,\n",
    "            'MSPE_bmk': MSPE_bmk,\n",
    "            'MSPE_model': MSPE_model,\n",
    "            'pred_bmk': pred_bmk,\n",
    "            'pred_model': pred_model,\n",
    "            'losses': losses,\n",
    "            'CW_test': cw_test,\n",
    "            'elapsed time': elapsed_time}\n",
    "    \n",
    "    return out\n",
    "\n",
    "### ========================================================= ###\n",
    "\n",
    "\n",
    "data_stocks = pd.read_csv('data_stocks.csv')\n",
    "x = np.array(data_stocks.drop(columns=['yyyymm', 'EQPREM']), dtype=np.float32)\n",
    "y = np.array(data_stocks.loc[:,['EQPREM']], dtype=np.float32)\n",
    "# ADD PAST EQPREM TO FEATURE MATRIX\n",
    "x = np.hstack([x, y])\n",
    "\n",
    "display = ['out-of-sample R2', 'MSPE_bmk', 'losses', 'CW_test', 'elapsed time']\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "'''\n",
    "\n",
    "model_args = {\n",
    "    'module': RNN,\n",
    "    'module__input_size': 13,\n",
    "    'module__hidden_size': 10,\n",
    "    'module__num_layers': 2,\n",
    "    'module__dropout': 0.5,\n",
    "    'criterion': torch.nn.MSELoss,\n",
    "    'optimizer': optim.SGD,\n",
    "    'max_epochs': 10,\n",
    "    'batch_size': 36,\n",
    "    'train_split': None,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "cv_args = {\n",
    "    'module__hidden_size':[10],\n",
    "    'module__num_layers':[2]\n",
    "}\n",
    "\n",
    "results = expanding_window_recurrent_skorch(x[0:408], \n",
    "                                         y[0:408], \n",
    "                                         360,\n",
    "                                         window_size=12,\n",
    "                                         cv=TimeSeriesSplit(2),\n",
    "                                         model_args=model_args,\n",
    "                                         cv_args=cv_args,\n",
    "                                         random_seeds=None,\n",
    "                                         verbosity=50,\n",
    "                                         workers=-1,\n",
    "                                         parallel=False,\n",
    "                                         standardize=False,\n",
    "                                         retrain_interval=12)\n",
    "\n",
    "for item in display:\n",
    "    print(f'{item}: {results[item]}')\n",
    "    \n",
    "results['pred_model']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ky-fF5_ekARy",
   "metadata": {
    "id": "Ky-fF5_ekARy"
   },
   "source": [
    "### Evaluate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "y9r8Qf0DR9-L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9r8Qf0DR9-L",
    "outputId": "e195346e-f2cf-4855-9315-59655f1f2073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  60 | elapsed:  6.2min remaining:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  60 | elapsed:  8.3min remaining:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  60 | elapsed:  8.5min remaining:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  60 | elapsed:  9.1min remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  60 | elapsed:  9.3min remaining:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  60 | elapsed:  9.7min remaining:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  60 | elapsed: 10.2min remaining:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  60 | elapsed: 10.4min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  60 | elapsed: 10.7min remaining:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  60 | elapsed: 12.2min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  60 | elapsed: 12.8min remaining:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  60 | elapsed: 13.3min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  60 | elapsed: 13.3min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  60 | elapsed: 13.5min remaining:   42.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 14.4min finished\n",
      "out-of-sample R2: -0.07412040982241552\n",
      "MSPE_bmk: 0.0017830463985747667\n",
      "losses: {'MSE': 0.0019156909045238775, 'MAE': 0.0003259723577697822, 'R2LOG': 2.4257962746089747}\n",
      "CW_test: -226.7432416945424\n",
      "elapsed time: 864.245991230011\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    'module': RNN,\n",
    "    'module__input_size': 13,\n",
    "    'module__hidden_size': 10,\n",
    "    'module__num_layers': 2,\n",
    "    'module__dropout': 0.1,\n",
    "    'criterion': torch.nn.MSELoss,\n",
    "    'optimizer': optim.SGD,\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 36,\n",
    "    'train_split': None,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "cv_args = {\n",
    "    'module__hidden_size':[10, 100],\n",
    "    'module__num_layers':[2, 3],\n",
    "    'lr': [0.1, 0.001]\n",
    "}\n",
    "\n",
    "results = expanding_window_recurrent_skorch(x[0:1080], \n",
    "                                         y[0:1080], \n",
    "                                         360,\n",
    "                                         window_size=12,\n",
    "                                         cv=TimeSeriesSplit(3),\n",
    "                                         model_args=model_args,\n",
    "                                         cv_args=cv_args,\n",
    "                                         random_seeds=range(60),\n",
    "                                         verbosity=50,\n",
    "                                         workers=-1,\n",
    "                                         parallel=True,\n",
    "                                         standardize=True,\n",
    "                                         retrain_interval=12)\n",
    "\n",
    "for item in display:\n",
    "    print(f'{item}: {results[item]}')\n",
    "\n",
    "with open('nn_results/results_RNN.pickle', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qsAdjjFkVmYW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsAdjjFkVmYW",
    "outputId": "ff899f08-a0c6-4199-8110-e4dd29ef8893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  60 | elapsed:  9.2min remaining:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  60 | elapsed: 12.4min remaining: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  60 | elapsed: 12.5min remaining:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  60 | elapsed: 13.1min remaining:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  60 | elapsed: 13.8min remaining:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  60 | elapsed: 14.4min remaining:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  60 | elapsed: 15.0min remaining:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  60 | elapsed: 15.4min remaining:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  60 | elapsed: 15.8min remaining:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  60 | elapsed: 17.7min remaining:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  60 | elapsed: 18.9min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  60 | elapsed: 19.0min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  60 | elapsed: 19.6min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  60 | elapsed: 20.5min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 20.8min finished\n",
      "out-of-sample R2: -0.03677700432233166\n",
      "MSPE_bmk: 0.0017830463985747667\n",
      "losses: {'MSE': 0.0018504428104286076, 'MAE': 0.00029586375291914744, 'R2LOG': 2.182031170493016}\n",
      "CW_test: -378.5136407240051\n",
      "elapsed time: 1249.3790802955627\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    'module': LSTM,\n",
    "    'module__input_size': 13,\n",
    "    'module__hidden_size': 10,\n",
    "    'module__num_layers': 2,\n",
    "    'module__dropout': 0.1,\n",
    "    'criterion': torch.nn.MSELoss,\n",
    "    'optimizer': optim.SGD,\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 36,\n",
    "    'train_split': None,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "cv_args = {\n",
    "    'module__hidden_size':[10, 100],\n",
    "    'module__num_layers':[2, 3],\n",
    "}\n",
    "\n",
    "\n",
    "results = expanding_window_recurrent_skorch(x[0:1080], \n",
    "                                         y[0:1080], \n",
    "                                         360,\n",
    "                                         window_size=12,\n",
    "                                         cv=TimeSeriesSplit(3),\n",
    "                                         model_args=model_args,\n",
    "                                         cv_args=cv_args,\n",
    "                                         random_seeds=range(60),\n",
    "                                         verbosity=50,\n",
    "                                         workers=-1,\n",
    "                                         parallel=True,\n",
    "                                         standardize=True,\n",
    "                                         retrain_interval=12)\n",
    "\n",
    "for item in display:\n",
    "    print(f'{item}: {results[item]}')\n",
    "\n",
    "with open('nn_results/results_LSTM.pickle', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7UQVKTLJbZjm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UQVKTLJbZjm",
    "outputId": "829fd3f4-ba80-47e2-e83d-cc15570c9ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  60 | elapsed:  6.0min remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  60 | elapsed:  8.4min remaining:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  60 | elapsed:  8.5min remaining:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  60 | elapsed:  8.9min remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  60 | elapsed:  9.3min remaining:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  60 | elapsed:  9.7min remaining:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  60 | elapsed:  9.8min remaining:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  60 | elapsed: 10.0min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  60 | elapsed: 10.4min remaining:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  60 | elapsed: 11.9min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  60 | elapsed: 12.7min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  60 | elapsed: 13.0min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  60 | elapsed: 13.2min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  60 | elapsed: 13.4min remaining:   42.2s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 14.0min finished\n",
      "out-of-sample R2: -0.07412040982241552\n",
      "MSPE_bmk: 0.0017830463985747667\n",
      "losses: {'MSE': 0.0019156909045238775, 'MAE': 0.0003259723577697822, 'R2LOG': 2.4257962746089747}\n",
      "CW_test: -226.7432416945424\n",
      "elapsed time: 840.6125276088715\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    'module': GRU,\n",
    "    'module__input_size': 13,\n",
    "    'module__hidden_size': 10,\n",
    "    'module__num_layers': 2,\n",
    "    'module__dropout': 0.1,\n",
    "    'criterion': torch.nn.MSELoss,\n",
    "    'optimizer': optim.SGD,\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 36,\n",
    "    'train_split': None,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "cv_args = {\n",
    "    'module__hidden_size':[10, 100],\n",
    "    'module__num_layers':[2, 3],\n",
    "    'lr': [0.1, 0.001]\n",
    "}\n",
    "\n",
    "\n",
    "results = expanding_window_recurrent_skorch(x[0:1080], \n",
    "                                         y[0:1080], \n",
    "                                         360,\n",
    "                                         window_size=12,\n",
    "                                         cv=TimeSeriesSplit(3),\n",
    "                                         model_args=model_args,\n",
    "                                         cv_args=cv_args,\n",
    "                                         random_seeds=range(60),\n",
    "                                         verbosity=50,\n",
    "                                         workers=-1,\n",
    "                                         parallel=True,\n",
    "                                         standardize=True,\n",
    "                                         retrain_interval=12)\n",
    "\n",
    "for item in display:\n",
    "    print(f'{item}: {results[item]}')\n",
    "\n",
    "with open('nn_results/results_GRU.pickle', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29be5c9",
   "metadata": {},
   "source": [
    "### DM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942154e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stocks = pd.read_csv('data_stocks.csv')\n",
    "\n",
    "X = np.array(data_stocks.drop(columns=['yyyymm', 'EQPREM']))\n",
    "y = np.array(data_stocks.loc[:,['EQPREM']]).flatten()\n",
    "\n",
    "X_train = X[0:360,:]\n",
    "X_test = X[360:1080,:]\n",
    "y_train = y[0:360]\n",
    "y_test = y[360:1080]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c064573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM expw: -3.9526942101850637\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "filename = 'nn_results/results_RNN.pickle'\n",
    "with open(filename, 'rb') as file:\n",
    "    model_results =  pickle.load(file)\n",
    "    pred_bmk = model_results['pred_bmk']\n",
    "    pred_model = model_results['pred_model']\n",
    "    results = DM_test(y_test, pred_bmk, pred_model)\n",
    "    print(f'DM expw: {results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b6d843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM expw: -2.5466903036473547\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "filename = 'nn_results/results_LSTM.pickle'\n",
    "with open(filename, 'rb') as file:\n",
    "    model_results =  pickle.load(file)\n",
    "    pred_bmk = model_results['pred_bmk']\n",
    "    pred_model = model_results['pred_model']\n",
    "    results = DM_test(y_test, pred_bmk, pred_model)\n",
    "    print(f'DM expw: {results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e31c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM expw: -2.5466903036473547\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "filename = 'nn_results/results_LSTM.pickle'\n",
    "with open(filename, 'rb') as file:\n",
    "    model_results =  pickle.load(file)\n",
    "    pred_bmk = model_results['pred_bmk']\n",
    "    pred_model = model_results['pred_model']\n",
    "    results = DM_test(y_test, pred_bmk, pred_model)\n",
    "    print(f'DM expw: {results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906a2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Network Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
